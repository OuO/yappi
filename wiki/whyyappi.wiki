#summary Why we implemented yet another python profiler and some techinal details.
=Motivation:=
_CPython_ standart distribution is coming with three profilers. _cProfile_, _Profile_ and _hotshot_. _cProfile_ module is implemented as a C module based on _lsprof_, _Profile_ is in pure Python and the hotshot can be seen as a small subset of a cProfile. The motivation to implement a new profiler is that all of these profilers lacks the support of multi-threaded programs. If you want to profile a multi-threaded application, you must give an entry point to these profilers and then maybe merge the outputs. None of these profilers is designed to work on long-running multi-threaded application. While implementing a game server, it turns out that is is impossible to profile an application retrieve the statistics then stop and then start later on on the fly(without affecting the profiled application). With the experience of implementing a game server in Python, we have identified most of the problems, tricky parts regarding profiler usage and so, we have come up with simple but powerful requirements.

=Requirements:=
  * Profiler should be started/stopped at any time from any thread in the application.
  * Profile statistics should be obtained from any thread at any time.
  * Profile events themselves should work theoretically in O(1) time and O(n+m) space(where m is recursion count) per any profile event. This means: application behavior/details/size should not affect the profiler run-time.
  * "Profiler pollution"(effect on the application run-time) should be very minimal and observable via statistics to see how much load the profiler is putting on the application. Best way to compute this pollution is to compute the percentage of total run time spent in profiler to the total application run-time.(these statistics shall also include space requirements per application)
  * Profiler shall handle the error/exception situations internally. For example running out of memory should be handled by the profiler itself. No OS/Python VM exception that occurred inside profiler shall not affect the profiled application.

=Technical:=
The theoretical O(1) behavior of yappi is based on various usage of hash tables for profiling data structures. Every profile event is mapped to a growing hash table(in which this growing factor can be changed) and then retrieved. ~~The hash table is using simple linked-list bucketing but with a minor change that every lookup for an object in the bucket moves it closer to the bucket head. This is because profilers have strong access patterns on same functions/methods~~. Efficient usage of hash tables is the key to implement a fast profiler, but of course it is not everything. *I am awaiting any hints or idea from community to reduce the processing time of the profiler or improve in other way that I am not aware.* (_TODO: WE NEED MORE INFORMATION HERE_)

Update: A simple free list is used to allocate/deallocate items. Static memory pools are created at profiler initialization and grows when necessary at runtime. This also decreases profiler runtime and moves profiler behavior more towards the theoretical O(1) limit. Pooling also makes space requirements of the profiled application predictable.

=Tested platforms:=
  * OpenSuse 11.1
  * Ubuntu 9.04
  * Windows XP SP2
Tests are performed with Python 2.5, 2.4 and 2.6 respectively.
Tests shall and will include big-endian and 64 bit machines.

=Limitations:=
Threads must be derived from "threading" module's Thread object.